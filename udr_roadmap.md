# Unified Data Runtime (UDR) Development Roadmap

## Executive Summary

UDR is building the next generation of data infrastructure‚Äîone system that replaces the fragmented landscape of transactional databases, data warehouses, streaming platforms, and feature stores. This is not incremental improvement; it's architectural unification.

**Current Progress:** Phases 1-3 complete, Phase 4 in progress. We have a working proof-of-concept with content-addressable storage, versioned catalogs, time travel queries, DuckDB integration, and Git-like branching (core complete, QueryEngine integration pending).

**Tech Stack:**
- **Core:** Rust (performance, safety, concurrency)
- **Bindings:** PyO3 (Python interoperability)
- **Query Engine:** DuckDB (via Python)
- **Hashing:** BLAKE3
- **Serialization:** Apache Arrow / Parquet

---

## Progress Overview

| Phase | Status | Key Deliverable |
|-------|--------|-----------------|
| Phase 0: Environment | ‚úÖ Complete | Rust + PyO3 working |
| Phase 1: Chunk Store | ‚úÖ Complete | Content-addressable storage with deduplication |
| Phase 2: Catalog | ‚úÖ Complete | Versioned tables with time travel |
| Phase 3: Query Layer | ‚úÖ Complete | DuckDB + SQL + time travel queries |
| Phase 4: Branching | üîÑ In Progress | Git-like branches (core complete, QueryEngine integration pending) |
| Phase 5: Transactions | ‚è≥ Planned | Cross-table ACID |
| Phase 6: Changelog | ‚è≥ Planned | Unified batch/stream |
| Phase 7: Production | ‚è≥ Planned | Real workload migration |
| Phase 8: Release | ‚è≥ Planned | Documentation and publication |

---

## ‚úÖ COMPLETED PHASES

### Phase 0: Environment & Rust Foundations ‚úÖ

**Status:** Complete

- [x] Rust toolchain installed and configured
- [x] VS Code with rust-analyzer
- [x] Cargo workspace with `udr_core` and `udr_python`
- [x] PyO3 bindings working (`maturin develop`)
- [x] Git repository initialized and pushed to GitHub

### Phase 1: Content-Addressable Chunk Store ‚úÖ

**Status:** Complete (22 Rust tests passing)

**What We Built:**
- [x] `ChunkStore` struct with BLAKE3 hashing
- [x] Hash-based path derivation (`ab/cd/abcdef...`)
- [x] Atomic writes (write-to-temp-rename pattern)
- [x] Hash validation (64 hex character format)
- [x] `get_verified()` with integrity checking
- [x] Deduplication (same content ‚Üí same hash ‚Üí single copy)
- [x] Python bindings (`PyChunkStore`)
- [x] Comprehensive error types (`HashMismatch`, `NotFound`, etc.)

**Key Files:**
- `udr_core/src/chunk_store/store.rs`
- `udr_core/src/chunk_store/error.rs`
- `udr_python/src/lib.rs`

### Phase 2: Table Catalog & Versioning ‚úÖ

**Status:** Complete

**What We Built:**
- [x] `TableVersion` struct with metadata
- [x] `FileCatalog` with JSON-based version storage
- [x] Sequential version enforcement
- [x] Time travel (`get_version(table, Some(v))`)
- [x] Latest version resolution
- [x] Atomic commits with latest pointer
- [x] `list_versions()` and `list_tables()`
- [x] Python bindings (`PyCatalog`, `PyTableVersion`)

**Key Files:**
- `udr_core/src/catalog/file_catalog.rs`
- `udr_core/src/catalog/version.rs`
- `udr_core/src/catalog/error.rs`

### Phase 3: Query Layer (DuckDB Integration) ‚úÖ

**Status:** Complete (26 Python tests passing, 46 total)

**What We Built:**
- [x] `TableWriter` - DataFrame/Arrow ‚Üí Parquet chunks ‚Üí catalog
- [x] `TableReader` - catalog ‚Üí chunks ‚Üí Arrow/DataFrame
- [x] `QueryEngine` - DuckDB wrapper with time travel
- [x] SQL queries over versioned tables
- [x] Time travel in queries (`versions={"table": 1}`)
- [x] `diff_versions()` for comparing versions
- [x] JOIN queries across multiple tables
- [x] Multiple result formats (pandas, Arrow, dict)
- [x] Chunking for large tables
- [x] Type stubs for IDE support (`udr.pyi`)

**Key Files:**
- `python/udr_query/writer.py`
- `python/udr_query/reader.py`
- `python/udr_query/engine.py`
- `tests/test_query_layer.py`

---

## üîÑ IN PROGRESS

### Phase 4: Branching

**Status:** Core implementation complete, QueryEngine integration pending

**Goal:** Git-like branching for data development workflows

**Why This Matters:**
- Experiment with data transformations without affecting production
- A/B test algorithm changes on real data
- Collaborate on data changes with isolated workspaces
- Zero storage cost until changes are made (content addressing!)

**Data Model:**
```
Branch:
  - name: String
  - head: HashMap<String, u64>  # table_name ‚Üí version
  - created_at: Timestamp
  - parent_branch: Option<String>
  - description: Option<String>
```

**Completed:**

- [x] **4.1** Define `Branch` struct in Rust
  - `udr_core/src/branch/branch.rs`
  - Includes `BranchDiff` for comparing branches

- [x] **4.2** Implement `BranchManager`
  - `udr_core/src/branch/manager.rs`
  - Full CRUD: create, get, list, delete
  - Head operations: update_head, get_table_version
  - Default branch management

- [x] **4.3** Implement branch diff
  - Compare head pointers
  - Report: unchanged, modified, added_in_source, added_in_target
  - Conflict detection

- [x] **4.4** Implement fast-forward merge
  - If target is ancestor of source: update pointers
  - If diverged: `BranchError::MergeConflict` with table list

- [x] **4.5** Zero-copy branch creation
  - Verified: 50 branches created in < 1 second
  - Branch metadata < 1KB per branch

- [x] **4.6** Add Python bindings
  ```python
  manager = udr.PyBranchManager("./data/branches")
  manager.create("feature/new-scoring", from_branch="main")
  manager.list()
  manager.get("feature/new-scoring")
  manager.diff("feature/new-scoring", "main")
  manager.merge("feature/new-scoring", into="main")
  ```

- [x] **4.7** Write tests (20 Python tests)
  - Branch creation and isolation
  - Head operations
  - Diff and merge
  - Zero-copy verification
  - Branch name validation

**Pending:**

- [ ] **4.8** QueryEngine integration
  - Add `branch_manager` parameter to `QueryEngine.__init__`
  - Add `current_branch` state (default: "main")
  - Add `checkout(branch_name)` method
  - Update `_ensure_registered()` to resolve via branch heads
  - Update `write_table()` to update branch heads
  - Add `branch` parameter to `query()` method

**Merge Strategy Evolution:**

| Phase | Strategy | Capability |
|-------|----------|------------|
| Phase 4 (now) | Fast-forward + conflict detection | Simple merges work; diverged branches report conflict |
| Phase 4.5 (future) | Three-way merge | Full merge with ancestor tracking |
| Phase 5+ | Transactional merge | Atomic cross-table merge with MVCC |

**Milestone Checkpoint:**
- [x] Create branch from main
- [x] Modify head pointers on branch
- [x] Verify branch isolation
- [x] Diff branches
- [x] Merge back to main (fast-forward)
- [x] Storage increase ‚âà 0 (verified)
- [ ] Query via branch-aware QueryEngine

---

## ‚è≥ PLANNED PHASES

### Phase 5: Cross-Table Transactions

**Goal:** ACID transactions across multiple tables

**Why This Matters:**
- Update customer + orders + audit log atomically
- No more saga patterns and compensation logic
- Snapshot isolation: readers never block

**Design Decisions:**
- **Concurrency Control:** Optimistic (MVCC)
- **Isolation Level:** Snapshot Isolation
- **Scope:** Single-process first, distributed later

**Key Components:**
1. `Transaction` struct with read/write sets
2. `TransactionManager` with conflict detection
3. Write-ahead logging (WAL) for recovery
4. Python context manager interface

**Example API:**
```python
with udr.transaction(store, catalog) as tx:
    parcels = tx.read_table("parcels")
    scores = tx.read_table("scores")

    # Modify
    tx.write_table("parcels", new_parcels)
    tx.write_table("scores", new_scores)

    # Commits atomically on exit
# Rolls back on exception
```

### Phase 6: Changelog & Subscriptions

**Goal:** Unified batch and streaming via changelog

**Why This Matters:**
- Batch query: "What is the state at version V?"
- Stream query: "What changed since version V?"
- Same data model, same guarantees

**Key Components:**
1. `ChangelogEntry` for each commit
2. `Subscriber` API for change notifications
3. `diff_versions()` for computing deltas

**Example API:**
```python
def on_change(entry):
    for change in entry.changes:
        if change.table_name == "parcels":
            print(f"Parcels: v{change.old_version} ‚Üí v{change.new_version}")

subscriber = udr.Subscriber(changelog, from_commit=100)
subscriber.subscribe(on_change)
```

### Phase 7: Production Migration

**Goal:** Run real workloads on UDR

**Key Steps:**
1. Audit current data storage and flows
2. Design UDR table schema
3. Zero-copy import from existing Parquet/Iceberg
4. Adapt queries to UDR
5. Parallel operation and validation
6. Cutover with rollback path

### Phase 8: Documentation & Release

**Goal:** Publishable, usable by others

**Deliverables:**
- Comprehensive README (done!)
- API documentation (Rust docs, Python docstrings)
- Tutorial notebooks
- Architecture document
- Demo video/GIF
- Blog post
- GitHub Actions CI/CD

---

## Scaling Strategy

### Horizontal Scaling Path

```
Phase 1-3: Single-node POC (Complete)
    ‚Üì
Phase 4-6: Single-node with full features ‚Üê WE ARE HERE
    ‚Üì
Phase 7: Production validation
    ‚Üì
Future: Distributed deployment
```

**Distributed Architecture (Future):**
- Consistent hashing for chunk distribution
- Raft consensus for catalog coordination
- Epoch-based transaction ordering
- Read replicas for query scaling

### Storage Scaling

| Scale | Strategy | Notes |
|-------|----------|-------|
| GB | Local filesystem | Current implementation |
| TB | Local SSD + cold storage | Tiered storage |
| PB | Distributed chunk store | S3/GCS backend |
| EB | Federated deployments | Multi-region |

### Query Scaling

| Scale | Strategy | Notes |
|-------|----------|-------|
| Simple | Single DuckDB | Current implementation |
| Complex | DuckDB + caching | Materialized views |
| Large | Distributed query | Trino/Spark integration |
| Real-time | Streaming layer | Kafka/Flink integration |

---

## Risk Mitigation

| Risk | Mitigation |
|------|------------|
| Complexity growth | Phase-based delivery with clear milestones |
| Performance issues | Benchmark early, profile before optimizing |
| Scope creep | "Nice to have" explicitly separated |
| Lock-in | Open standards (Arrow, Parquet, SQL) |
| Single point of failure | Content addressing enables federation |

---

## Mathematical Foundations (Proven)

From the whitepaper verification:

| Property | Complexity | Verified |
|----------|------------|----------|
| Write | O(n) in data size | ‚úÖ |
| Read | O(n) in data size | ‚úÖ |
| Time travel | O(1) version lookup | ‚úÖ |
| Branch creation | O(k) where k = tables | ‚úÖ |
| Collision probability | ~10^-47 at exabyte scale | ‚úÖ |
| Deduplication ratio | 60-85% typical | ‚úÖ |

---

## Decision Log

### 2026-01-16: Phase 4 Branching Design
**Decision:** Git-style branch names with slashes, implicit "main" branch, fast-forward merge with evolution to three-way merge
**Rationale:**
- Slash-based names (e.g., `feature/scoring-v2`) are familiar to developers
- Implicit "main" branch ensures backward compatibility with branchless code
- Fast-forward merge is simple and sufficient for POC; three-way merge added after proving viability
**Alternatives:**
- Hyphen-only names (rejected for familiarity)
- Explicit branch required (rejected for backward compatibility)
- Full three-way merge now (rejected for complexity)

### 2026-01-16: Phase 3 Architecture
**Decision:** Build query layer in Python with DuckDB, not Rust
**Rationale:** Faster iteration, better ecosystem integration, DuckDB handles heavy lifting
**Alternatives:** DataFusion (Rust-native), direct Parquet reading

### 2026-01-16: Phase 1 Hardening
**Decision:** Add hash validation and atomic writes before Phase 3
**Rationale:** Foundation must be solid before building query layer
**Alternatives:** Move fast, fix later (rejected for robustness)

### 2026-01-16: PyO3 0.23 Upgrade
**Decision:** Upgrade from PyO3 0.20 to 0.23
**Rationale:** Better ergonomics, `Bound<'py, T>` API, future-proof
**Alternatives:** Stay on 0.20 (rejected due to deprecation warnings)

---

## Quick Reference

**Build Commands:**
```bash
# Rust
cargo build --release
cargo test --all

# Python bindings
cd udr_python && maturin develop --release

# Python tests
pytest tests/ -v
```

**Key Imports:**
```python
import udr
from udr_query import TableWriter, TableReader, QueryEngine
```

**Test Counts:**
- Rust: 39 tests (22 core + 17 branch)
- Python: 66 tests (20 core + 26 query layer + 20 branching)

---

*Last updated: January 2026*
