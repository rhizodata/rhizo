#!/usr/bin/env python3
"""
Competitive Benchmark: Rhizo vs Delta Lake vs Iceberg

This benchmark compares core operations across data lakehouse formats.
Results are for internal analysis - understand the gaps and opportunities.
"""

import time
import tempfile
import shutil
import os
from dataclasses import dataclass, field
from typing import Optional
import json

import pandas as pd
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq

# Rhizo
import rhizo
import _rhizo
from rhizo import QueryEngine

# Delta Lake
import deltalake
from deltalake import DeltaTable, write_deltalake

# Note: PyIceberg requires a catalog (REST, Hive, etc.) - skip for now
# We'll compare Rhizo vs Delta Lake primarily


@dataclass
class BenchmarkResult:
    operation: str
    system: str
    duration_ms: float
    rows: int
    size_mb: float
    throughput_mb_s: Optional[float] = None
    notes: str = ""


def generate_test_data(rows: int) -> pd.DataFrame:
    """Generate consistent test data."""
    np.random.seed(42)
    return pd.DataFrame({
        "id": range(rows),
        "name": [f"user_{i}" for i in range(rows)],
        "value": np.random.random(rows),
        "category": np.random.choice(["A", "B", "C", "D"], rows),
        "timestamp": pd.date_range("2024-01-01", periods=rows, freq="s"),
    })


def benchmark_rhizo_write(df: pd.DataFrame, path: str) -> BenchmarkResult:
    """Benchmark Rhizo write."""
    store = _rhizo.PyChunkStore(os.path.join(path, "chunks"))
    catalog = _rhizo.PyCatalog(os.path.join(path, "catalog"))
    engine = QueryEngine(store, catalog)

    start = time.perf_counter()
    engine.write_table("benchmark", df)
    duration = (time.perf_counter() - start) * 1000

    size_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
    throughput = size_mb / (duration / 1000) if duration > 0 else 0

    return BenchmarkResult(
        operation="write",
        system="Rhizo",
        duration_ms=duration,
        rows=len(df),
        size_mb=size_mb,
        throughput_mb_s=throughput,
    )


def benchmark_delta_write(df: pd.DataFrame, path: str) -> BenchmarkResult:
    """Benchmark Delta Lake write."""
    delta_path = os.path.join(path, "delta_table")

    start = time.perf_counter()
    write_deltalake(delta_path, df, mode="overwrite")
    duration = (time.perf_counter() - start) * 1000

    size_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
    throughput = size_mb / (duration / 1000) if duration > 0 else 0

    return BenchmarkResult(
        operation="write",
        system="Delta Lake",
        duration_ms=duration,
        rows=len(df),
        size_mb=size_mb,
        throughput_mb_s=throughput,
    )


def benchmark_rhizo_read(path: str) -> BenchmarkResult:
    """Benchmark Rhizo read."""
    store = _rhizo.PyChunkStore(os.path.join(path, "chunks"))
    catalog = _rhizo.PyCatalog(os.path.join(path, "catalog"))
    engine = QueryEngine(store, catalog)

    start = time.perf_counter()
    result = engine.query("SELECT * FROM benchmark")
    df: pd.DataFrame = result.to_pandas()
    duration = (time.perf_counter() - start) * 1000

    size_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
    throughput = size_mb / (duration / 1000) if duration > 0 else 0

    return BenchmarkResult(
        operation="read",
        system="Rhizo",
        duration_ms=duration,
        rows=len(df),
        size_mb=size_mb,
        throughput_mb_s=throughput,
    )


def benchmark_delta_read(path: str) -> BenchmarkResult:
    """Benchmark Delta Lake read."""
    delta_path = os.path.join(path, "delta_table")

    start = time.perf_counter()
    dt = DeltaTable(delta_path)
    df = dt.to_pandas()
    duration = (time.perf_counter() - start) * 1000

    size_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
    throughput = size_mb / (duration / 1000) if duration > 0 else 0

    return BenchmarkResult(
        operation="read",
        system="Delta Lake",
        duration_ms=duration,
        rows=len(df),
        size_mb=size_mb,
        throughput_mb_s=throughput,
    )


def benchmark_rhizo_versioning(df: pd.DataFrame, path: str, num_versions: int) -> BenchmarkResult:
    """Benchmark Rhizo multi-version write."""
    store = _rhizo.PyChunkStore(os.path.join(path, "chunks"))
    catalog = _rhizo.PyCatalog(os.path.join(path, "catalog"))
    engine = QueryEngine(store, catalog)

    start = time.perf_counter()
    for i in range(num_versions):
        # Modify 5% of rows each version
        modified = df.copy()
        rows_to_change = np.random.choice(len(df), len(df) // 20, replace=False)
        modified.loc[rows_to_change, "value"] = np.random.random(len(rows_to_change))
        engine.write_table("benchmark", modified)
    duration = (time.perf_counter() - start) * 1000

    # Check storage size
    chunks_dir = os.path.join(path, "chunks")
    total_size = sum(
        os.path.getsize(os.path.join(dirpath, filename))
        for dirpath, _, filenames in os.walk(chunks_dir)
        for filename in filenames
    )
    storage_mb = total_size / 1024 / 1024

    return BenchmarkResult(
        operation=f"write_{num_versions}_versions",
        system="Rhizo",
        duration_ms=duration,
        rows=len(df) * num_versions,
        size_mb=storage_mb,
        notes=f"Storage for {num_versions} versions with 5% change each",
    )


def benchmark_delta_versioning(df: pd.DataFrame, path: str, num_versions: int) -> BenchmarkResult:
    """Benchmark Delta Lake multi-version write."""
    delta_path = os.path.join(path, "delta_table")

    # Initial write
    write_deltalake(delta_path, df, mode="overwrite")

    start = time.perf_counter()
    for i in range(num_versions - 1):
        # Modify 5% of rows each version
        modified = df.copy()
        rows_to_change = np.random.choice(len(df), len(df) // 20, replace=False)
        modified.loc[rows_to_change, "value"] = np.random.random(len(rows_to_change))
        write_deltalake(delta_path, modified, mode="overwrite")
    duration = (time.perf_counter() - start) * 1000

    # Check storage size
    total_size = sum(
        os.path.getsize(os.path.join(dirpath, filename))
        for dirpath, _, filenames in os.walk(delta_path)
        for filename in filenames
    )
    storage_mb = total_size / 1024 / 1024

    return BenchmarkResult(
        operation=f"write_{num_versions}_versions",
        system="Delta Lake",
        duration_ms=duration,
        rows=len(df) * num_versions,
        size_mb=storage_mb,
        notes=f"Storage for {num_versions} versions with 5% change each",
    )


def benchmark_rhizo_time_travel(path: str, version: int) -> BenchmarkResult:
    """Benchmark Rhizo time travel query."""
    store = _rhizo.PyChunkStore(os.path.join(path, "chunks"))
    catalog = _rhizo.PyCatalog(os.path.join(path, "catalog"))
    engine = QueryEngine(store, catalog)

    start = time.perf_counter()
    result = engine.query("SELECT * FROM benchmark", versions={"benchmark": version})
    df: pd.DataFrame = result.to_pandas()
    duration = (time.perf_counter() - start) * 1000

    return BenchmarkResult(
        operation=f"time_travel_v{version}",
        system="Rhizo",
        duration_ms=duration,
        rows=len(df),
        size_mb=df.memory_usage(deep=True).sum() / 1024 / 1024,
    )


def benchmark_delta_time_travel(path: str, version: int) -> BenchmarkResult:
    """Benchmark Delta Lake time travel query."""
    delta_path = os.path.join(path, "delta_table")

    start = time.perf_counter()
    dt = DeltaTable(delta_path, version=version)
    df = dt.to_pandas()
    duration = (time.perf_counter() - start) * 1000

    return BenchmarkResult(
        operation=f"time_travel_v{version}",
        system="Delta Lake",
        duration_ms=duration,
        rows=len(df),
        size_mb=df.memory_usage(deep=True).sum() / 1024 / 1024,
    )


def benchmark_rhizo_branching(df: pd.DataFrame, path: str) -> BenchmarkResult:
    """Benchmark Rhizo branch creation."""
    store = _rhizo.PyChunkStore(os.path.join(path, "chunks"))
    catalog = _rhizo.PyCatalog(os.path.join(path, "catalog"))
    branches = _rhizo.PyBranchManager(os.path.join(path, "branches"))
    engine = QueryEngine(store, catalog, branch_manager=branches)

    # Write initial data
    engine.write_table("benchmark", df)

    start = time.perf_counter()
    branches.create("feature-branch", from_branch="main")
    duration = (time.perf_counter() - start) * 1000

    # Check branch storage overhead
    branches_dir = os.path.join(path, "branches")
    branch_size = sum(
        os.path.getsize(os.path.join(dirpath, filename))
        for dirpath, _, filenames in os.walk(branches_dir)
        for filename in filenames
    )

    return BenchmarkResult(
        operation="create_branch",
        system="Rhizo",
        duration_ms=duration,
        rows=len(df),
        size_mb=branch_size / 1024 / 1024,
        notes="Zero-copy branch creation",
    )


def benchmark_delta_branching(df: pd.DataFrame, path: str) -> BenchmarkResult:
    """Delta Lake doesn't have native branching - simulate with copy."""
    delta_path = os.path.join(path, "delta_table")
    branch_path = os.path.join(path, "delta_table_branch")

    # Write initial data
    write_deltalake(delta_path, df, mode="overwrite")

    start = time.perf_counter()
    # Delta doesn't have branching - would need to copy the table
    shutil.copytree(delta_path, branch_path)
    duration = (time.perf_counter() - start) * 1000

    # Check branch storage
    branch_size = sum(
        os.path.getsize(os.path.join(dirpath, filename))
        for dirpath, _, filenames in os.walk(branch_path)
        for filename in filenames
    )

    return BenchmarkResult(
        operation="create_branch",
        system="Delta Lake",
        duration_ms=duration,
        rows=len(df),
        size_mb=branch_size / 1024 / 1024,
        notes="Full copy (no native branching)",
    )


def run_comparison_benchmarks():
    """Run all comparison benchmarks."""
    results = []

    print("=" * 80)
    print("Rhizo vs Delta Lake Benchmark")
    print("=" * 80)
    print()

    # Test different data sizes
    for row_count in [10_000, 100_000, 500_000]:
        print(f"\n### {row_count:,} rows ###\n")
        df = generate_test_data(row_count)
        data_size_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
        print(f"Data size: {data_size_mb:.2f} MB")

        with tempfile.TemporaryDirectory() as tmpdir:
            rhizo_path = os.path.join(tmpdir, "rhizo")
            delta_path = os.path.join(tmpdir, "delta")
            os.makedirs(rhizo_path)
            os.makedirs(delta_path)

            # Write benchmarks
            print("\n--- Write Performance ---")
            r1 = benchmark_rhizo_write(df, rhizo_path)
            r2 = benchmark_delta_write(df, delta_path)
            results.extend([r1, r2])
            print(f"Rhizo: {r1.duration_ms:.1f}ms ({r1.throughput_mb_s:.1f} MB/s)")
            print(f"Delta Lake: {r2.duration_ms:.1f}ms ({r2.throughput_mb_s:.1f} MB/s)")

            # Read benchmarks
            print("\n--- Read Performance ---")
            r3 = benchmark_rhizo_read(rhizo_path)
            r4 = benchmark_delta_read(delta_path)
            results.extend([r3, r4])
            print(f"Rhizo: {r3.duration_ms:.1f}ms ({r3.throughput_mb_s:.1f} MB/s)")
            print(f"Delta Lake: {r4.duration_ms:.1f}ms ({r4.throughput_mb_s:.1f} MB/s)")

    # Versioning benchmark (10 versions, 5% change each)
    print("\n" + "=" * 80)
    print("### Versioning Benchmark (10 versions, 5% change each) ###")
    print("=" * 80)

    df = generate_test_data(100_000)
    with tempfile.TemporaryDirectory() as tmpdir:
        rhizo_path = os.path.join(tmpdir, "rhizo")
        delta_path = os.path.join(tmpdir, "delta")
        os.makedirs(rhizo_path)
        os.makedirs(delta_path)

        r5 = benchmark_rhizo_versioning(df, rhizo_path, 10)
        r6 = benchmark_delta_versioning(df, delta_path, 10)
        results.extend([r5, r6])

        print(f"\nRhizo: {r5.duration_ms:.1f}ms, Storage: {r5.size_mb:.2f} MB")
        print(f"Delta Lake: {r6.duration_ms:.1f}ms, Storage: {r6.size_mb:.2f} MB")

        theoretical_naive = df.memory_usage(deep=True).sum() / 1024 / 1024 * 10
        print(f"\nNaive storage (10 full copies): {theoretical_naive:.2f} MB")
        print(f"Rhizo dedup ratio: {(1 - r5.size_mb / theoretical_naive) * 100:.1f}%")
        print(f"Delta dedup ratio: {(1 - r6.size_mb / theoretical_naive) * 100:.1f}%")

    # Time travel benchmark
    print("\n" + "=" * 80)
    print("### Time Travel Benchmark ###")
    print("=" * 80)

    df = generate_test_data(100_000)
    with tempfile.TemporaryDirectory() as tmpdir:
        rhizo_path = os.path.join(tmpdir, "rhizo")
        delta_path = os.path.join(tmpdir, "delta")
        os.makedirs(rhizo_path)
        os.makedirs(delta_path)

        # Create multiple versions
        store = _rhizo.PyChunkStore(os.path.join(rhizo_path, "chunks"))
        catalog = _rhizo.PyCatalog(os.path.join(rhizo_path, "catalog"))
        engine = QueryEngine(store, catalog)

        write_deltalake(os.path.join(delta_path, "delta_table"), df, mode="overwrite")

        for i in range(5):
            modified = df.copy()
            modified["value"] = np.random.random(len(df))
            engine.write_table("benchmark", modified)
            write_deltalake(
                os.path.join(delta_path, "delta_table"),
                modified,
                mode="overwrite"
            )

        print("\nQuerying version 1 (oldest):")
        r7 = benchmark_rhizo_time_travel(rhizo_path, 1)
        r8 = benchmark_delta_time_travel(delta_path, 0)
        results.extend([r7, r8])
        print(f"Rhizo: {r7.duration_ms:.1f}ms")
        print(f"Delta Lake: {r8.duration_ms:.1f}ms")

        print("\nQuerying version 5 (latest):")
        r9 = benchmark_rhizo_time_travel(rhizo_path, 5)
        r10 = benchmark_delta_time_travel(delta_path, 4)
        results.extend([r9, r10])
        print(f"Rhizo: {r9.duration_ms:.1f}ms")
        print(f"Delta Lake: {r10.duration_ms:.1f}ms")

    # Branching benchmark
    print("\n" + "=" * 80)
    print("### Branching Benchmark ###")
    print("=" * 80)

    df = generate_test_data(100_000)
    with tempfile.TemporaryDirectory() as tmpdir:
        rhizo_path = os.path.join(tmpdir, "rhizo")
        delta_path = os.path.join(tmpdir, "delta")
        os.makedirs(rhizo_path)
        os.makedirs(delta_path)

        r11 = benchmark_rhizo_branching(df, rhizo_path)
        r12 = benchmark_delta_branching(df, delta_path)
        results.extend([r11, r12])

        print(f"\nRhizo: {r11.duration_ms:.1f}ms, Branch size: {r11.size_mb * 1024:.0f} bytes")
        print(f"Delta Lake: {r12.duration_ms:.1f}ms, Branch size: {r12.size_mb:.2f} MB (full copy)")
        print(f"\nRhizo branch overhead: {r11.size_mb * 1024:.0f} bytes")
        print(f"Delta Lake branch overhead: {r12.size_mb * 1024 * 1024:.0f} bytes")
        print(f"Ratio: {r12.size_mb / r11.size_mb if r11.size_mb > 0 else 'N/A'}x more storage for Delta")

    # Summary
    print("\n" + "=" * 80)
    print("SUMMARY: Feature Comparison")
    print("=" * 80)
    print("""
| Feature                  | Rhizo | Delta Lake | Winner      |
|--------------------------|------------|------------|-------------|
| Single-table ACID        | Yes        | Yes        | Tie         |
| Cross-table ACID         | Yes        | No         | Rhizo  |
| Time Travel              | Yes        | Yes        | Tie         |
| Zero-copy Branching      | Yes        | No         | Rhizo  |
| Content Deduplication    | Yes        | No         | Rhizo  |
| Incremental Dedup        | Yes (95%)  | No         | Rhizo  |
| Built-in Integrity       | Yes        | No         | Rhizo  |
| Cloud Storage (S3/GCS)   | No*        | Yes        | Delta Lake  |
| Ecosystem/Adoption       | New        | Mature     | Delta Lake  |
| Production Hardening     | Months     | Years      | Delta Lake  |

* Rhizo cloud storage planned but not yet implemented
""")

    return results


def save_results(results: list, filepath: str):
    """Save results to JSON for analysis."""
    data = [
        {
            "operation": r.operation,
            "system": r.system,
            "duration_ms": r.duration_ms,
            "rows": r.rows,
            "size_mb": r.size_mb,
            "throughput_mb_s": r.throughput_mb_s,
            "notes": r.notes,
        }
        for r in results
    ]
    with open(filepath, "w") as f:
        json.dump(data, f, indent=2)
    print(f"\nResults saved to {filepath}")


if __name__ == "__main__":
    results = run_comparison_benchmarks()
    save_results(results, "comparison_results.json")
