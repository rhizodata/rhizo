# Unified Data Runtime (UDR)

**A next-generation data infrastructure that unifies transactional, analytical, and streaming workloads through content-addressable storage, cross-table ACID transactions, and Git-like versioning.**

[![Rust Tests](https://img.shields.io/badge/tests-110%20passed-brightgreen)]()
[![Python Tests](https://img.shields.io/badge/python%20tests-109%20passed-brightgreen)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)]()

## Why UDR?

Modern data infrastructure is fundamentally fragmented. Organizations maintain separate systems for:
- Transactional workloads (PostgreSQL, MySQL)
- Analytical processing (Snowflake, BigQuery)
- Streaming computation (Kafka, Flink)
- Feature stores for ML pipelines

**The result:** 60-80% of data engineering effort goes to integration, synchronization, and pipeline maintenance rather than generating insights.

UDR eliminates this fragmentation through five foundational innovations:

| Innovation | Benefit |
|------------|---------|
| **Content-Addressable Storage** | Automatic deduplication, corruption detection, zero-copy operations |
| **Cross-Table ACID Transactions** | Atomic updates across your entire data estate |
| **Time Travel** | Query any historical version instantly |
| **Git-like Branching** | Experiment safely, merge when ready |
| **Unified Batch/Stream** | One semantic model for all workloads |

## Current Status

| Phase | Status | Description |
|-------|--------|-------------|
| Phase 1: Storage | âœ… Complete | Content-addressable chunk store with BLAKE3 hashing |
| Phase 2: Catalog | âœ… Complete | Versioned file catalog with time travel |
| Phase 3: Query | âœ… Complete | DuckDB integration with SQL + time travel |
| Phase 4: Branching | âœ… Complete | Git-like branching with zero-copy semantics |
| Phase 5: Transactions | ðŸš§ In Progress | Cross-table ACID with MVCC (Phase 5.0-5.1 complete, Recovery remaining) |
| Phase 6: Changelog | â³ Planned | Unified batch/stream via subscriptions |

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Application Layer                       â”‚
â”‚          Python (udr_query) â”‚ Rust â”‚ CLI (planned)          â”‚
â”‚      TableWriter â”‚ TableReader â”‚ QueryEngine (DuckDB)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FileCatalog                           â”‚
â”‚   â€¢ Versioned table metadata                                 â”‚
â”‚   â€¢ Sequential version enforcement                           â”‚
â”‚   â€¢ Time travel queries                                      â”‚
â”‚   â€¢ Atomic commits                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ChunkStore                            â”‚
â”‚   â€¢ Content-addressed storage (BLAKE3)                       â”‚
â”‚   â€¢ Automatic deduplication                                  â”‚
â”‚   â€¢ Atomic writes (write-to-temp-rename)                     â”‚
â”‚   â€¢ Integrity verification (get_verified)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       File System                            â”‚
â”‚   â€¢ 2-level directory tree for O(1) lookups (ab/cd/...)     â”‚
â”‚   â€¢ JSON metadata files                                      â”‚
â”‚   â€¢ Parquet data chunks                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Prerequisites
- Rust 1.70+ (`rustup install stable`)
- Python 3.9+
- maturin (`pip install maturin`)

### Installation

```bash
# Clone and build
git clone https://github.com/yourusername/unifieddataruntime.git
cd unifieddataruntime

# Build Rust core and Python bindings
cd udr_python && maturin develop --release && cd ..

# Install Python dependencies
pip install pyarrow duckdb pandas pytest
```

### Basic Usage (Python)

```python
import udr
from udr_query import TableWriter, TableReader, QueryEngine
import pandas as pd

# Initialize storage
store = udr.PyChunkStore("./data/chunks")
catalog = udr.PyCatalog("./data/catalog")

# Write data using the query layer
engine = QueryEngine(store, catalog)

df = pd.DataFrame({
    "id": [1, 2, 3],
    "name": ["Alice", "Bob", "Charlie"],
    "score": [85.5, 92.0, 78.5]
})

engine.write_table("users", df)

# SQL queries with DuckDB
result = engine.query("SELECT * FROM users WHERE score > 80")
print(result.to_pandas())
```

### Time Travel

```python
# Write version 1
engine.write_table("users", df_v1)

# Write version 2 with updated data
df_v2 = df_v1.copy()
df_v2["score"] = df_v2["score"] + 10
engine.write_table("users", df_v2)

# Query any historical version
result_v1 = engine.query(
    "SELECT AVG(score) FROM users",
    versions={"users": 1}  # Time travel!
)

result_v2 = engine.query(
    "SELECT AVG(score) FROM users",
    versions={"users": 2}
)

# Compare versions
diff = engine.diff_versions("users", 1, 2, key_columns=["id"])
print(f"Rows added: {diff['rows_added']}")
print(f"Rows removed: {diff['rows_removed']}")
```

### Branching (Phase 4)

```python
import udr
from udr_query import QueryEngine
import pandas as pd

# Initialize with branch support
store = udr.PyChunkStore("./data/chunks")
catalog = udr.PyCatalog("./data/catalog")
branches = udr.PyBranchManager("./data/branches")
engine = QueryEngine(store, catalog, branch_manager=branches)

# Write initial data to main branch
df = pd.DataFrame({"id": [1, 2], "score": [80, 90]})
engine.write_table("scores", df)  # Automatically updates main branch head

# Create a feature branch (zero-copy - instant!)
engine.create_branch("feature/new-scoring", description="Testing new algorithm")

# Switch to feature branch
engine.checkout("feature/new-scoring")
print(f"Current branch: {engine.current_branch}")

# Modify data on feature branch
df_updated = pd.DataFrame({"id": [1, 2], "score": [85, 95]})
engine.write_table("scores", df_updated)

# Query both branches - see different results!
main_avg = engine.query("SELECT AVG(score) as avg FROM scores", branch="main")
feat_avg = engine.query("SELECT AVG(score) as avg FROM scores", branch="feature/new-scoring")
print(f"Main avg: {main_avg.to_pandas()['avg'].iloc[0]}")      # 85.0
print(f"Feature avg: {feat_avg.to_pandas()['avg'].iloc[0]}")   # 90.0

# Compare branches
diff = engine.diff_branches("feature/new-scoring", "main")
print(f"Modified tables: {diff['modified']}")  # [("scores", 2, 1)]

# Merge when ready
engine.checkout("main")
engine.merge_branch("feature/new-scoring", into="main")
```

### Transactions (Phase 5)

```python
import udr
from udr_query import QueryEngine
import pandas as pd

# Initialize with transaction support
store = udr.PyChunkStore("./data/chunks")
catalog = udr.PyCatalog("./data/catalog")
branches = udr.PyBranchManager("./data/branches")
tx_manager = udr.PyTransactionManager("./data/transactions", "./data/catalog", "./data/branches")

engine = QueryEngine(
    store, catalog,
    branch_manager=branches,
    transaction_manager=tx_manager
)

# Write initial data
engine.write_table("users", pd.DataFrame({"id": [1, 2], "name": ["Alice", "Bob"]}))

# Cross-table ACID transaction
with engine.transaction() as tx:
    # Read current data (snapshot isolation)
    users = tx.query("SELECT * FROM users")

    # Buffer multiple writes (not visible until commit)
    tx.write_table("users", pd.DataFrame({"id": [1, 2, 3], "name": ["Alice", "Bob", "Charlie"]}))
    tx.write_table("audit_log", pd.DataFrame({"action": ["added_user"], "user_id": [3]}))

    # Read-your-writes: see buffered data within transaction
    count = tx.query("SELECT COUNT(*) as cnt FROM users")
    print(f"Users in transaction: {count.to_pandas()['cnt'].iloc[0]}")  # 3

    # Auto-commits on exit, rolls back on exception
# Both tables updated atomically!

# Verify commit
result = engine.query("SELECT COUNT(*) as cnt FROM users")
print(f"Committed users: {result.to_pandas()['cnt'].iloc[0]}")  # 3
```

### Low-Level API (Rust Core)

```python
import udr

# Content-addressable storage
store = udr.PyChunkStore("./data/chunks")

# Store data - returns BLAKE3 hash
hash_str = store.put(b"Hello, World!")
print(f"Stored with hash: {hash_str}")

# Retrieve with integrity verification
data = store.get_verified(hash_str)  # Raises if corrupted

# Versioned catalog
catalog = udr.PyCatalog("./data/catalog")

version = udr.PyTableVersion("users", 1, [hash_str])
catalog.commit(version)

# Time travel to any version
v1 = catalog.get_version("users", 1)
latest = catalog.get_version("users")  # Gets latest
```

## Testing

```bash
# Run all Rust tests (110 tests)
cargo test --all

# Run Python tests (81 tests)
pytest tests/ -v
```

## Project Structure

```
unifieddataruntime/
â”œâ”€â”€ Cargo.toml                    # Workspace configuration
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ udr_roadmap.md               # Development roadmap
â”œâ”€â”€ unified_data_runtime_whitepaper.docx  # Technical whitepaper
â”‚
â”œâ”€â”€ udr_core/                     # Rust core library
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ lib.rs               # Public exports
â”‚       â”œâ”€â”€ chunk_store/         # Content-addressable storage
â”‚       â”‚   â”œâ”€â”€ store.rs         # ChunkStore (BLAKE3, atomic writes)
â”‚       â”‚   â””â”€â”€ error.rs         # ChunkStoreError, HashMismatch
â”‚       â”œâ”€â”€ catalog/             # Versioned catalog
â”‚       â”‚   â”œâ”€â”€ file_catalog.rs  # FileCatalog implementation
â”‚       â”‚   â”œâ”€â”€ version.rs       # TableVersion struct
â”‚       â”‚   â””â”€â”€ error.rs         # CatalogError types
â”‚       â”œâ”€â”€ branch/              # Git-like branching (Phase 4)
â”‚       â”‚   â”œâ”€â”€ branch.rs        # Branch, BranchDiff structs
â”‚       â”‚   â”œâ”€â”€ manager.rs       # BranchManager (create, merge, diff)
â”‚       â”‚   â””â”€â”€ error.rs         # BranchError types
â”‚       â””â”€â”€ transaction/         # Cross-table ACID (Phase 5)
â”‚           â”œâ”€â”€ types.rs         # TxId, TransactionRecord, WriteGranularity
â”‚           â”œâ”€â”€ epoch.rs         # EpochConfig, EpochMetadata
â”‚           â”œâ”€â”€ error.rs         # TransactionError types
â”‚           â”œâ”€â”€ log.rs           # TransactionLog (persistent WAL)
â”‚           â”œâ”€â”€ conflict.rs      # ConflictDetector trait, TableLevelConflictDetector
â”‚           â”œâ”€â”€ manager.rs       # TransactionManager (coordinator)
â”‚           â””â”€â”€ recovery.rs      # RecoveryManager, RecoveryReport
â”‚
â”œâ”€â”€ udr_python/                   # PyO3 bindings
â”‚   â””â”€â”€ src/lib.rs               # Python interface
â”‚
â”œâ”€â”€ python/                       # Python packages
â”‚   â”œâ”€â”€ udr.pyi                  # Type stubs for IDE support
â”‚   â””â”€â”€ udr_query/               # Query layer
â”‚       â”œâ”€â”€ writer.py            # TableWriter (DataFrame â†’ Parquet chunks)
â”‚       â”œâ”€â”€ reader.py            # TableReader (chunks â†’ DataFrame)
â”‚       â””â”€â”€ engine.py            # QueryEngine (DuckDB + time travel)
â”‚
â”œâ”€â”€ tests/                        # Test suites
â”‚   â”œâ”€â”€ test_udr.py              # Core Rust binding tests (20 tests)
â”‚   â”œâ”€â”€ test_query_layer.py      # Query layer tests (26 tests)
â”‚   â”œâ”€â”€ test_branching.py        # Branching tests (20 tests)
â”‚   â””â”€â”€ test_branch_query_integration.py  # Branch+Query integration (15 tests)
â”‚
â””â”€â”€ examples/
    â””â”€â”€ time_travel_demo.py      # Interactive demo
```

## Design Principles

1. **Immutability**: All data is immutable once written. Updates create new versions.
2. **Content Addressing**: Data identified by BLAKE3 hash enables automatic deduplication.
3. **Atomic Operations**: Write-to-temp-rename pattern prevents corruption.
4. **Layered Architecture**: ChunkStore, FileCatalog, and BranchManager are independent and composable.
5. **Time Travel by Default**: Every version is preserved and queryable.
6. **Zero-Copy Branching**: Branches are pointers to table versions, not data copies.

## Performance Characteristics

| Operation | Complexity | Notes |
|-----------|------------|-------|
| Write chunk | O(n) | n = data size; hashing + file write |
| Read chunk | O(n) | n = data size; direct file read |
| Check exists | O(1) | Hash â†’ path lookup |
| Time travel query | O(1) | Direct version lookup, no log replay |
| Commit version | O(k) | k = number of chunk hashes |
| Branch creation | O(k) | k = tables; zero data copy |

## Theoretical Impact

Based on the technical whitepaper analysis:

| Metric | Current State | With UDR |
|--------|--------------|----------|
| Data engineering overhead | 60-80% on integration | Focus on insights |
| Storage efficiency | Multiple copies per system | 60-85% deduplication |
| Query consistency | Eventually consistent | ACID across tables |
| Historical queries | Limited or impossible | Instant time travel |
| Experimentation | Risky, requires copies | Zero-copy branching |

## Roadmap

See [udr_roadmap.md](./udr_roadmap.md) for the complete development roadmap.

**Next milestone (Phase 5.1: Transaction Integration):**
- Python context manager for transactions
- QueryEngine integration with transaction support
- Read-your-writes within transaction scope
- Automatic rollback on exceptions

## References

- [Technical Whitepaper](./unified_data_runtime_whitepaper.docx) - Full architectural specification
- [BLAKE3](https://github.com/BLAKE3-team/BLAKE3) - Cryptographic hash function
- [DuckDB](https://duckdb.org/) - Analytical SQL engine
- [Apache Arrow](https://arrow.apache.org/) - Columnar memory format
- [PyO3](https://pyo3.rs/) - Rust bindings for Python

## License

MIT

## Contributing

Contributions welcome! Please read the development roadmap for planned features and architecture decisions.
